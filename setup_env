#!/bin/bash

export HF_HOME=/home/work/conan/.cache/hf

sudo apt update && sudo apt install tmux

curl -fsSL https://ollama.com/install.sh | sh

python -m pip install --upgrade pip
pip install poetry
poetry lock && poetry install
pip install -U https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.2cxx11abiTRUE-cp312-cp312-linux_x86_64.whl

git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp && make LLAMA_CUBLAS=1 
# pip install deepspeed-kernels
# DS_BUILD_OPS=1 pip install deepspeed --global-option="build_ext" --global-option="-j8"
